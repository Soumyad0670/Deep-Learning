{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a617da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bfa02881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dda04dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f1b6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences\n",
    "sent = [\"The cat sat on the mat\",\n",
    "       \"The dog sat on the log\",\n",
    "       \"The cat chased the dog\",\n",
    "       \"The dog chased the cat\",\n",
    "       \"The cat is on the mat\",\n",
    "       \"The dog is on the log\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "261650a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['The cat sat on the mat', 'The dog sat on the log', 'The cat chased the dog', 'The dog chased the cat', 'The cat is on the mat', 'The dog is on the log']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences:\", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded documents: [[263, 124, 118, 448, 263, 118], [263, 186, 118, 448, 263, 487], [263, 124, 367, 263, 186], [263, 186, 367, 263, 124], [263, 124, 52, 448, 263, 118], [263, 186, 52, 448, 263, 487]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 500\n",
    "embedding_dim = 8\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in sent]\n",
    "print(\"Encoded documents:\", encoded_docs)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "def5f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7b67a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded documents: [[263 124 118 448 263 118]\n",
      " [263 186 118 448 263 487]\n",
      " [263 124 367 263 186   0]\n",
      " [263 186 367 263 124   0]\n",
      " [263 124  52 448 263 118]\n",
      " [263 186  52 448 263 487]]\n",
      "Shape: (6, 6)\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Dtype: int32\n",
      "Model input shape: (None, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_26\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_26\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_23 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m8\u001b[0m)           │         \u001b[38;5;34m4,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,000</span> (15.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,000\u001b[0m (15.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,000</span> (15.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,000\u001b[0m (15.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001849120FC40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Predictions : [[[ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [ 0.01083471  0.04049388  0.04018494  0.01642385  0.042056\n",
      "    0.02672154 -0.00908234 -0.01454563]\n",
      "  [-0.03027741  0.01186644  0.00740627  0.01894921  0.02543484\n",
      "    0.00685992  0.03083852 -0.00233655]\n",
      "  [-0.03340769  0.04343922 -0.00359309 -0.04478893  0.00782824\n",
      "   -0.02284473  0.0221597  -0.04920521]\n",
      "  [ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [-0.03027741  0.01186644  0.00740627  0.01894921  0.02543484\n",
      "    0.00685992  0.03083852 -0.00233655]]\n",
      "\n",
      " [[ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [-0.01853503 -0.00414215 -0.01859139  0.00373037  0.00402814\n",
      "    0.02712656  0.0422035  -0.03378342]\n",
      "  [-0.03027741  0.01186644  0.00740627  0.01894921  0.02543484\n",
      "    0.00685992  0.03083852 -0.00233655]\n",
      "  [-0.03340769  0.04343922 -0.00359309 -0.04478893  0.00782824\n",
      "   -0.02284473  0.0221597  -0.04920521]\n",
      "  [ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [ 0.03125801 -0.00734893 -0.03030726  0.0210575  -0.00917657\n",
      "   -0.01777489  0.01551348  0.00826295]]\n",
      "\n",
      " [[ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [ 0.01083471  0.04049388  0.04018494  0.01642385  0.042056\n",
      "    0.02672154 -0.00908234 -0.01454563]\n",
      "  [-0.03228092  0.02775968  0.02856422 -0.02179729  0.00808879\n",
      "   -0.03637978 -0.01957625 -0.00931055]\n",
      "  [ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [-0.01853503 -0.00414215 -0.01859139  0.00373037  0.00402814\n",
      "    0.02712656  0.0422035  -0.03378342]\n",
      "  [-0.04934815  0.04177665 -0.02062768  0.0421715   0.03014084\n",
      "   -0.03111707 -0.02409576  0.04380795]]\n",
      "\n",
      " [[ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [-0.01853503 -0.00414215 -0.01859139  0.00373037  0.00402814\n",
      "    0.02712656  0.0422035  -0.03378342]\n",
      "  [-0.03228092  0.02775968  0.02856422 -0.02179729  0.00808879\n",
      "   -0.03637978 -0.01957625 -0.00931055]\n",
      "  [ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [ 0.01083471  0.04049388  0.04018494  0.01642385  0.042056\n",
      "    0.02672154 -0.00908234 -0.01454563]\n",
      "  [-0.04934815  0.04177665 -0.02062768  0.0421715   0.03014084\n",
      "   -0.03111707 -0.02409576  0.04380795]]\n",
      "\n",
      " [[ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [ 0.01083471  0.04049388  0.04018494  0.01642385  0.042056\n",
      "    0.02672154 -0.00908234 -0.01454563]\n",
      "  [-0.03114326 -0.03604984 -0.04992436 -0.03444527  0.00422771\n",
      "   -0.02707808  0.02981201 -0.03837436]\n",
      "  [-0.03340769  0.04343922 -0.00359309 -0.04478893  0.00782824\n",
      "   -0.02284473  0.0221597  -0.04920521]\n",
      "  [ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [-0.03027741  0.01186644  0.00740627  0.01894921  0.02543484\n",
      "    0.00685992  0.03083852 -0.00233655]]\n",
      "\n",
      " [[ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [-0.01853503 -0.00414215 -0.01859139  0.00373037  0.00402814\n",
      "    0.02712656  0.0422035  -0.03378342]\n",
      "  [-0.03114326 -0.03604984 -0.04992436 -0.03444527  0.00422771\n",
      "   -0.02707808  0.02981201 -0.03837436]\n",
      "  [-0.03340769  0.04343922 -0.00359309 -0.04478893  0.00782824\n",
      "   -0.02284473  0.0221597  -0.04920521]\n",
      "  [ 0.01931331  0.00428424 -0.01305189 -0.02390587 -0.007769\n",
      "    0.02829227  0.02104032  0.01189115]\n",
      "  [ 0.03125801 -0.00734893 -0.03030726  0.0210575  -0.00917657\n",
      "   -0.01777489  0.01551348  0.00826295]]]\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 6\n",
    "# Pad the encoded_docs to the desired sequence length\n",
    "encoded_docs = pad_sequences(encoded_docs, maxlen=sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Check shape and type\n",
    "print(\"Encoded documents:\", encoded_docs)\n",
    "print(\"Shape:\", encoded_docs.shape)      # Should be (6, 6)\n",
    "print(\"Type:\", type(encoded_docs))       # Must be <class 'numpy.ndarray'>\n",
    "print(\"Dtype:\", encoded_docs.dtype)      # Should be int32 or int64\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=6))\n",
    "model.compile('adam', 'mse')\n",
    "model.build(input_shape=(None, sequence_length))  # None is the batch size\n",
    "\n",
    "model.build(input_shape=(None, 6))  # None is the batch size\n",
    "print(\"Model input shape:\", model.input_shape)\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n",
    "\n",
    "import numpy as np\n",
    "predictions = model.predict(encoded_docs)\n",
    "print(\"Predictions :\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0643c666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded documents:\n",
      " [[  0   0   0   0 263 124 118 448 263 118]\n",
      " [  0   0   0   0 263 186 118 448 263 487]\n",
      " [  0   0   0   0 263 124 367 263 186   0]\n",
      " [  0   0   0   0 263 186 367 263 124   0]\n",
      " [  0   0   0   0 263 124  52 448 263 118]\n",
      " [  0   0   0   0 263 186  52 448 263 487]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "[[[-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [ 0.04093403  0.02504433 -0.04828462  0.03210217  0.01914693\n",
      "    0.0165066  -0.03343976  0.04481992]\n",
      "  [ 0.03956303  0.04497666  0.02045503  0.02737847 -0.04921323\n",
      "   -0.01625142 -0.01656647 -0.04254183]\n",
      "  [-0.01642507  0.03351576 -0.019441    0.04637371  0.04592302\n",
      "   -0.04675225  0.04578534 -0.01159452]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [ 0.03956303  0.04497666  0.02045503  0.02737847 -0.04921323\n",
      "   -0.01625142 -0.01656647 -0.04254183]]\n",
      "\n",
      " [[-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [-0.00614887 -0.04683394  0.02736575 -0.0409494  -0.00722118\n",
      "   -0.00717566 -0.03776593  0.01676864]\n",
      "  [ 0.03956303  0.04497666  0.02045503  0.02737847 -0.04921323\n",
      "   -0.01625142 -0.01656647 -0.04254183]\n",
      "  [-0.01642507  0.03351576 -0.019441    0.04637371  0.04592302\n",
      "   -0.04675225  0.04578534 -0.01159452]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [-0.0240958   0.04978709  0.02012265  0.02315233  0.01932829\n",
      "   -0.03960101  0.0295922   0.03961993]]\n",
      "\n",
      " [[-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [ 0.04093403  0.02504433 -0.04828462  0.03210217  0.01914693\n",
      "    0.0165066  -0.03343976  0.04481992]\n",
      "  [-0.01864336  0.01423594 -0.02573892  0.04755416 -0.0212324\n",
      "   -0.04533179  0.02433033 -0.00142497]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [-0.00614887 -0.04683394  0.02736575 -0.0409494  -0.00722118\n",
      "   -0.00717566 -0.03776593  0.01676864]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]]\n",
      "\n",
      " [[-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [-0.00614887 -0.04683394  0.02736575 -0.0409494  -0.00722118\n",
      "   -0.00717566 -0.03776593  0.01676864]\n",
      "  [-0.01864336  0.01423594 -0.02573892  0.04755416 -0.0212324\n",
      "   -0.04533179  0.02433033 -0.00142497]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [ 0.04093403  0.02504433 -0.04828462  0.03210217  0.01914693\n",
      "    0.0165066  -0.03343976  0.04481992]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]]\n",
      "\n",
      " [[-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [ 0.04093403  0.02504433 -0.04828462  0.03210217  0.01914693\n",
      "    0.0165066  -0.03343976  0.04481992]\n",
      "  [-0.02720169  0.03079145  0.04174316  0.00563189  0.01347872\n",
      "   -0.04926201  0.03225159  0.02368078]\n",
      "  [-0.01642507  0.03351576 -0.019441    0.04637371  0.04592302\n",
      "   -0.04675225  0.04578534 -0.01159452]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [ 0.03956303  0.04497666  0.02045503  0.02737847 -0.04921323\n",
      "   -0.01625142 -0.01656647 -0.04254183]]\n",
      "\n",
      " [[-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [-0.00614887 -0.04683394  0.02736575 -0.0409494  -0.00722118\n",
      "   -0.00717566 -0.03776593  0.01676864]\n",
      "  [-0.02720169  0.03079145  0.04174316  0.00563189  0.01347872\n",
      "   -0.04926201  0.03225159  0.02368078]\n",
      "  [-0.01642507  0.03351576 -0.019441    0.04637371  0.04592302\n",
      "   -0.04675225  0.04578534 -0.01159452]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [-0.0240958   0.04978709  0.02012265  0.02315233  0.01932829\n",
      "   -0.03960101  0.0295922   0.03961993]]]\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences\n",
    "max_length = 10\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='pre')\n",
    "print(\"Padded documents:\\n\", padded_docs)\n",
    "print(model.predict(padded_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "41ea3368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of the embedding layer: [[-0.04672387  0.0252513   0.04160093 ...  0.02982745  0.04926676\n",
      "  -0.00516806]\n",
      " [-0.02158055 -0.03347833  0.02351132 ... -0.03240325  0.03982097\n",
      "   0.04199858]\n",
      " [-0.03907311 -0.03378366  0.00565916 ...  0.00543913 -0.02293836\n",
      "  -0.00706404]\n",
      " ...\n",
      " [-0.04957249  0.00115796  0.01973024 ... -0.03969352  0.04025329\n",
      "   0.00548764]\n",
      " [ 0.00519142 -0.00477131  0.00737673 ...  0.00783898 -0.04526198\n",
      "   0.01504571]\n",
      " [-0.0418165   0.00328505  0.00252652 ...  0.03386971 -0.02408072\n",
      "  -0.02200813]]\n",
      "Embedding vector for word index 1: [-0.02158055 -0.03347833  0.02351132  0.00551087 -0.02174786 -0.03240325\n",
      "  0.03982097  0.04199858]\n",
      "WARNING:tensorflow:5 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000184912567A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Embedding for sentence index 0: [[[-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.04672387  0.0252513   0.04160093  0.03644267 -0.04437103\n",
      "    0.02982745  0.04926676 -0.00516806]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [ 0.04093403  0.02504433 -0.04828462  0.03210217  0.01914693\n",
      "    0.0165066  -0.03343976  0.04481992]\n",
      "  [ 0.03956303  0.04497666  0.02045503  0.02737847 -0.04921323\n",
      "   -0.01625142 -0.01656647 -0.04254183]\n",
      "  [-0.01642507  0.03351576 -0.019441    0.04637371  0.04592302\n",
      "   -0.04675225  0.04578534 -0.01159452]\n",
      "  [-0.03248789  0.02002846  0.02507769 -0.00597906  0.00212635\n",
      "    0.00443172 -0.00149425  0.01508865]\n",
      "  [ 0.03956303  0.04497666  0.02045503  0.02737847 -0.04921323\n",
      "   -0.01625142 -0.01656647 -0.04254183]]]\n"
     ]
    }
   ],
   "source": [
    "# Get the weights of the embedding layer\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "print(\"Weights of the embedding layer:\", weights)\n",
    "\n",
    "# Get the embedding for a specific word\n",
    "word_index = 1  # Example word index\n",
    "embedding_vector = weights[word_index]\n",
    "print(f\"Embedding vector for word index {word_index}:\", embedding_vector)\n",
    "\n",
    "# Get the embedding for a specific sentence\n",
    "sentence_index = 0  # Example sentence index\n",
    "sentence_embedding = model.predict(padded_docs[sentence_index].reshape(1, -1))\n",
    "print(f\"Embedding for sentence index {sentence_index}:\", sentence_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
